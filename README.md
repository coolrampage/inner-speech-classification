Does the self-attention features of a transformer neural network allow it to capture long-range dependencies in EEG data when classifying inner speech better than a more traditional LSTM?
