Does the self-attention features of a transformer neural network allow it to capture long-range dependencies in EEG data when classifying inner speech better than a more traditional LSTM?

This code supplements the paper on "The Efficacy of Transformer Neural Networks and Long Short-Term Memory (LSTM) Networks to Classify Inner Speech using Electroencephalography (EEG) Data"
